{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What this script does\n",
    "\n",
    "This script lists and crawls news articles from [NAVER news](http://news.naver.com) service. Given the first hierarchical category (`sid1`), it shows the available sub-categories (`sid2`) and let you to choose which categories you intend to crawl. Currently, the list of `sid2`s, with `sid1=100` (Politics) is as follows:\n",
    "\n",
    "```\n",
    "{\n",
    " 264: '청와대 ', # The blue house\n",
    " 265: '국회/정당 ', # National assembly\n",
    " 268: '북한 ', # North Korea\n",
    " 266: '행정 ', # Administration\n",
    " 267: '국방/외교 ', # Military force\n",
    " 269: '정치일반 ' # General politics\n",
    "}\n",
    "```\n",
    "\n",
    "You can also set the time duration to crawl:\n",
    "\n",
    "```\n",
    "sid2s = [269]\n",
    "date_from = '20170101'\n",
    "date_to = '20170102'\n",
    "```\n",
    "\n",
    "this setting crawls all \"general politics\" news articles from Jan 01, 2017 to Jan 02, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "from urllib.parse import urlparse,parse_qs\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'http://news.naver.com/main/list.nhn?mode=LS2D&mid=shm&sid1=%d&sid2=%d&date=%s&page=%d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid1 = 100\n",
    "FETCH_CATEGORY_URL = 'http://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_from_url(url):\n",
    "    html = requests.get(url).content\n",
    "    time.sleep(0.1) # Give naver server some rest...\n",
    "    \n",
    "    return BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_soup = parse_from_url(FETCH_CATEGORY_URL)\n",
    "ul_tag = category_soup.find('ul', class_='nav')\n",
    "anchors = ul_tag.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {}\n",
    "\n",
    "for anchor in anchors:\n",
    "    parsed_query = parse_qs(anchor['href'])\n",
    "    if 'sid2' not in parsed_query:\n",
    "        continue\n",
    "    categories[int(parsed_query['sid2'][0])] = anchor.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{264: '청와대 ', 265: '국회/정당 ', 268: '북한 ', 266: '행정 ', 267: '국방/외교 ', 269: '정치일반 '}\n"
     ]
    }
   ],
   "source": [
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid2s = [269]\n",
    "date_from = '20160101'\n",
    "date_to = '20161231'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate list of dates between `from` and `to`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_from = datetime.datetime.strptime(date_from, \"%Y%m%d\")\n",
    "datetime_to = datetime.datetime.strptime(date_to, \"%Y%m%d\")\n",
    "datetime_generated = [datetime_from + datetime.timedelta(days=x) for x in range(0, (datetime_to - datetime_from).days)]\n",
    "date_generated = [d.strftime('%Y%m%d') for d in datetime_generated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl list of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = {}\n",
    "articles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in tqdm(date_generated, desc='date'): # date\n",
    "    for sid2 in sid2s: # 2nd category\n",
    "        for page in tqdm(range(1, 1000), desc='page', leave=False):\n",
    "            list_url = BASE_URL % (sid1, sid2, date, page)\n",
    "            list_soup = parse_from_url(list_url)\n",
    "            lists[list_url] = list_soup\n",
    "            \n",
    "            current_page_number = list_soup.find('div', class_='paging').find('strong').text\n",
    "            if int(current_page_number) != page:\n",
    "                break\n",
    "            \n",
    "            headline_list = list_soup.find('ul', class_='type06_headline')\n",
    "            headlines = headline_list.find_all('dl')\n",
    "            for headline in headlines:\n",
    "                title_anchor = headline.find('a')\n",
    "                writing = headline.find('span', class_='writing')\n",
    "                dt = headline.find('span', class_='date')\n",
    "                \n",
    "                title_text = title_anchor.text.strip()\n",
    "                if len(title_text) == 0:\n",
    "                    title_text = title_anchor.find('img')['alt'].strip()\n",
    "                \n",
    "                articles[title_anchor['href']] = {\n",
    "                    'title': title_text,\n",
    "                    'writing': writing.text.strip(),\n",
    "                    'datetime': dt.text.strip()\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save article list to JSON\n",
    "\n",
    "JSON format is faster and safer than pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('articles.json', 'w') as fw:\n",
    "    json.dump(articles, fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load article list from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('articles.json') as fp:\n",
    "    articles = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197372"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'JTBC',\n",
       " 'KBS 뉴스',\n",
       " 'MBC 뉴스',\n",
       " 'MBN',\n",
       " 'SBS CNBC',\n",
       " 'SBS 뉴스',\n",
       " 'TV조선',\n",
       " 'YTN',\n",
       " 'ZDNet Korea',\n",
       " '강원일보',\n",
       " '경향신문',\n",
       " '국민의당',\n",
       " '국민일보',\n",
       " '노컷뉴스',\n",
       " '뉴스1',\n",
       " '뉴시스',\n",
       " '더불어민주당',\n",
       " '데일리안',\n",
       " '동아일보',\n",
       " '디지털타임스',\n",
       " '마이데일리',\n",
       " '매일경제',\n",
       " '매일신문',\n",
       " '머니S',\n",
       " '머니투데이',\n",
       " '문화일보',\n",
       " '미디어오늘',\n",
       " '부산일보',\n",
       " '서울경제',\n",
       " '서울신문',\n",
       " '세계일보',\n",
       " '스포츠경향',\n",
       " '스포츠동아',\n",
       " '스포츠서울',\n",
       " '스포츠조선',\n",
       " '시사IN',\n",
       " '신동아',\n",
       " '아시아경제',\n",
       " '아이뉴스24',\n",
       " '엑스포츠뉴스',\n",
       " '여성신문',\n",
       " '연합뉴스',\n",
       " '연합뉴스TV',\n",
       " '오마이뉴스',\n",
       " '이데일리',\n",
       " '전자신문',\n",
       " '정의당',\n",
       " '정책브리핑',\n",
       " '조선비즈',\n",
       " '조세일보',\n",
       " '주간동아',\n",
       " '중앙SUNDAY',\n",
       " '중앙일보',\n",
       " '참세상',\n",
       " '채널A',\n",
       " '파이낸셜뉴스',\n",
       " '프레시안',\n",
       " '한겨레',\n",
       " '한겨레21',\n",
       " '한국경제',\n",
       " '한국경제TV',\n",
       " '한국일보',\n",
       " '헤럴드POP',\n",
       " '헤럴드경제'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([articles[article_url]['writing'] for article_url in articles])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4550386662f84cc88ef0b61373d3d1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=197372), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "articles_temp = []\n",
    "shuffled_article_urls = list(articles.keys())\n",
    "random.shuffle(shuffled_article_urls)\n",
    "\n",
    "for article_url in tqdm(shuffled_article_urls):\n",
    "    article_html = parse_from_url(article_url)\n",
    "    try:\n",
    "        article_text = article_html.find(\n",
    "            'div',\n",
    "            id='articleBodyContents'\n",
    "        )\n",
    "        if article_text is None:\n",
    "            continue\n",
    "        [s.extract() for s in article_text('script')]\n",
    "        \n",
    "        new_article = {\n",
    "            'url': article_url,\n",
    "            'title': articles[article_url]['title'],\n",
    "            'writing': articles[article_url]['writing'],\n",
    "            'datetime': articles[article_url]['datetime'],\n",
    "            'text': article_text.text.strip(),\n",
    "        }\n",
    "        articles_temp.append(new_article)\n",
    "        \n",
    "        if len(articles_temp) % 100 == 0:\n",
    "            with open('articles/%s.json' % str(time.time()), 'w') as fw:\n",
    "                json.dump(articles_temp, fw)\n",
    "            articles_temp = []\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(article_url)\n",
    "        print(e)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
